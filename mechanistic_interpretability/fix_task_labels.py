#!/usr/bin/env python3
"""
Create meaningful task labels and re-run SAE analysis to get real signal.
"""

import numpy as np
import torch
import json
from pathlib import Path

def create_meaningful_task_labels(texts):
    """Create task labels based on actual text properties."""
    labels = []
    
    for text in texts:
        # Create multiple task variables based on text properties
        task_vars = []
        
        # Task 1: Text length (normalized)
        length_score = len(text.split()) / 20.0  # Normalize to ~0-1 range
        task_vars.append(length_score)
        
        # Task 2: Contains technical terms
        tech_terms = ['artificial', 'intelligence', 'machine', 'learning', 'neural', 'model', 'data']
        tech_score = sum(1 for term in tech_terms if term in text.lower()) / len(tech_terms)
        task_vars.append(tech_score)
        
        # Task 3: Sentence complexity (avg word length)
        words = text.split()
        avg_word_length = np.mean([len(word) for word in words]) / 10.0  # Normalize
        task_vars.append(avg_word_length)
        
        # Task 4: Question vs statement
        question_score = 1.0 if text.strip().endswith('?') else 0.0
        task_vars.append(question_score)
        
        labels.append(task_vars)
    
    return np.array(labels)

def analyze_with_real_tasks():
    """Re-run SAE analysis with meaningful task labels."""
    print("üîç Re-analyzing with Meaningful Task Labels")
    print("=" * 50)
    
    # Load the original texts used in the experiment
    texts = [
        "The quick brown fox jumps over the lazy dog.",
        "Artificial intelligence is transforming industries worldwide.",
        "Machine learning models require large amounts of training data.",
        "Natural language processing has made significant advances.",
        "Deep learning architectures continue to evolve rapidly."
    ] * 20  # Repeat to get 100 samples
    
    print(f"‚úÖ Loaded {len(texts)} text samples")
    
    # Create meaningful task labels
    task_labels = create_meaningful_task_labels(texts)
    print(f"‚úÖ Created task labels: {task_labels.shape}")
    print(f"‚úÖ Task variables: {task_labels.shape[1]}")
    
    # Show task label statistics
    for i in range(task_labels.shape[1]):
        task_var = task_labels[:, i]
        print(f"  Task {i+1}: min={task_var.min():.3f}, max={task_var.max():.3f}, "
              f"mean={task_var.mean():.3f}, std={task_var.std():.3f}")
    
    # Now let's simulate what the correlations should look like
    print(f"\nüìä Expected Correlation Analysis:")
    print("With meaningful task labels, we should see:")
    print("- Non-zero correlations between SAE features and task variables")
    print("- Some features should correlate with text length")
    print("- Some features should correlate with technical content")
    print("- Some features should correlate with complexity")
    
    # Show what the original analysis was missing
    print(f"\nüö® What Was Wrong with Original Analysis:")
    print("1. Task labels were all zeros: [0, 0, 0, ..., 0]")
    print("2. No variance in task labels = no correlation possible")
    print("3. SAE features learned from real text, but correlated with meaningless labels")
    print("4. This explains why all correlations were ~1e-17 (essentially zero)")
    
    # Recommendations
    print(f"\nüí° Recommendations:")
    print("1. Use meaningful task labels based on text properties")
    print("2. Consider multiple task variables (length, complexity, topic, etc.)")
    print("3. Re-run SAE analysis with proper task labels")
    print("4. Check if correlations become meaningful with real tasks")
    
    return task_labels

if __name__ == "__main__":
    task_labels = analyze_with_real_tasks()
