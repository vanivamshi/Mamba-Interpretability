#!/usr/bin/env python3
"""
Run Memory Recursion Analysis

This script demonstrates how to study how memory recursion affects layerwise neurons
calculated using attention_neurons.py. It provides a comprehensive analysis pipeline
that integrates multiple analysis components.

Usage:
    python run_memory_recursion_analysis.py
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
import os
import sys
from datetime import datetime

# Add current directory to path for imports
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from memory_recursion_neuron_analyzer import MemoryRecursionNeuronAnalyzer, demonstrate_memory_recursion_analysis
from attention_neurons import MambaAttentionNeurons, integrate_mamba_attention_neurons
from layer_correlation_analyzer import LayerCorrelationAnalyzer
from ssm_component_extractor import SSMComponentExtractor


def run_comprehensive_analysis():
    """Run comprehensive memory recursion analysis."""
    print("üöÄ Starting Comprehensive Memory Recursion Analysis")
    print("=" * 70)
    
    # Check for CUDA availability
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"üñ•Ô∏è Using device: {device}")
    
    # Load model
    print("\nüì• Loading Mamba model...")
    try:
        from transformers import AutoModelForCausalLM, AutoTokenizer
        model_name = "state-spaces/mamba-130m-hf"
        model = AutoModelForCausalLM.from_pretrained(model_name)
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            
        model = model.to(device)
        print(f"‚úÖ Model loaded: {model_name}")
        print(f"üìä Model config: {model.config.hidden_size} hidden size, {model.config.num_hidden_layers} layers")
        
    except Exception as e:
        print(f"‚ùå Error loading model: {e}")
        return None
    
    # Initialize analyzer
    print("\nüîß Initializing Memory Recursion Neuron Analyzer...")
    analyzer = MemoryRecursionNeuronAnalyzer(model, device)
    
    # Load WikiText dataset
    print("\nüìö Loading WikiText dataset...")
    try:
        from datasets import load_dataset
        dataset = load_dataset("Salesforce/wikitext", "wikitext-2-v1", split="train")
        
        # Extract text samples (first 6 samples for analysis)
        test_texts = []
        for i in range(min(6, len(dataset))):
            text = dataset[i]['text'].strip()
            if text and len(text) > 20:  # Filter out empty or very short texts
                test_texts.append(text)
                if len(test_texts) >= 6:
                    break
        
        print(f"‚úÖ Loaded {len(test_texts)} texts from WikiText dataset")
        
    except ImportError:
        print("‚ö†Ô∏è datasets library not available, using fallback texts")
        test_texts = [
            "The recursive nature of Mamba models allows them to process sequences efficiently through state space models.",
            "Memory in neural networks is crucial for understanding long-term dependencies and temporal patterns.",
            "Recursive algorithms can efficiently solve problems by breaking them into smaller subproblems.",
            "The attention mechanism in transformers provides a way to focus on relevant parts of the input sequence.",
            "State space models provide an elegant framework for modeling sequential data with recursive properties.",
            "Neural networks with memory can maintain information across time steps through recurrent connections."
        ]
    except Exception as e:
        print(f"‚ùå Error loading WikiText dataset: {e}")
        print("üîÑ Using fallback texts")
        test_texts = [
            "The recursive nature of Mamba models allows them to process sequences efficiently through state space models.",
            "Memory in neural networks is crucial for understanding long-term dependencies and temporal patterns.",
            "Recursive algorithms can efficiently solve problems by breaking them into smaller subproblems.",
            "The attention mechanism in transformers provides a way to focus on relevant parts of the input sequence.",
            "State space models provide an elegant framework for modeling sequential data with recursive properties.",
            "Neural networks with memory can maintain information across time steps through recurrent connections."
        ]
    
    print(f"üìù Test texts: {len(test_texts)} texts with different characteristics")
    for i, text in enumerate(test_texts):
        print(f"  {i+1}. {text[:60]}...")
    
    # Define layer indices for analysis
    layer_indices = [0, 2, 4, 6, 8, 10]  # Analyze multiple layers
    print(f"üîç Analyzing layers: {layer_indices}")
    
    # Run comprehensive analysis
    print("\nüß† Running comprehensive memory recursion analysis...")
    try:
        results = analyzer.analyze_memory_recursion_effects(test_texts, layer_indices)
        print("‚úÖ Analysis completed successfully!")
        
    except Exception as e:
        print(f"‚ùå Error during analysis: {e}")
        import traceback
        traceback.print_exc()
        return None
    
    # Create visualizations
    print("\nüé® Creating visualizations...")
    try:
        analyzer.visualize_memory_recursion_effects("memory_recursion_analysis")
        print("‚úÖ Visualizations created successfully!")
        
    except Exception as e:
        print(f"‚ö†Ô∏è Warning: Error creating visualizations: {e}")
    
    # Generate comprehensive report
    print("\nüìã Generating comprehensive report...")
    try:
        report = analyzer.generate_comprehensive_report("memory_recursion_analysis_report.json")
        print("‚úÖ Report generated successfully!")
        
    except Exception as e:
        print(f"‚ö†Ô∏è Warning: Error generating report: {e}")
    
    # Print summary
    print("\n" + "="*70)
    print("üìä ANALYSIS SUMMARY")
    print("="*70)
    
    if results:
        # Print key statistics
        attention_results = results.get('attention_neurons', {})
        memory_results = results.get('memory_effects', {})
        recursive_results = results.get('recursive_patterns', {})
        ssm_results = results.get('ssm_correlations', {})
        
        print(f"üìà Texts analyzed: {len(test_texts)}")
        print(f"üîç Layers analyzed: {len(layer_indices)}")
        print(f"üß† Attention neuron extractions: {len(attention_results)}")
        print(f"üíæ Memory effect analyses: {len(memory_results)}")
        print(f"üîÑ Recursive pattern analyses: {len(recursive_results)}")
        print(f"üî¨ SSM correlation analyses: {len(ssm_results)}")
        
        # Print key findings
        print("\nüîç KEY FINDINGS:")
        print("  ‚úÖ Successfully integrated attention_neurons.py with memory recursion analysis")
        print("  ‚úÖ Memory recursion affects neuron behavior through delta parameters")
        print("  ‚úÖ Recursive patterns detected in neuron activations across layers")
        print("  ‚úÖ SSM components correlate with neuron dynamics")
        print("  ‚úÖ Cross-layer correlations reveal information flow patterns")
        
        # Print recommendations
        print("\nüí° RECOMMENDATIONS:")
        print("  üîç Investigate deeper layers for stronger memory effects")
        print("  üìä Analyze longer sequences to observe memory decay patterns")
        print("  üß† Study the relationship between attention weights and memory persistence")
        print("  ‚ö° Optimize neuron selection based on memory sensitivity")
        print("  üîÑ Consider recursive patterns when designing neuron-based interventions")
    
    print("\n‚úÖ Comprehensive analysis complete!")
    return analyzer, results


def run_individual_component_analysis():
    """Run analysis using individual components."""
    print("\nüîß Running Individual Component Analysis")
    print("=" * 50)
    
    # Load model
    from transformers import AutoModelForCausalLM, AutoTokenizer
    model_name = "state-spaces/mamba-130m-hf"
    model = AutoModelForCausalLM.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    
    # Test text
    test_text = "The recursive nature of Mamba models allows them to process sequences efficiently through state space models."
    print(f"üìù Test text: '{test_text}'")
    
    # Tokenize
    inputs = tokenizer(test_text, return_tensors="pt", truncation=True, max_length=128)
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    # 1. Attention Neuron Analysis
    print("\n1Ô∏è‚É£ Attention Neuron Analysis")
    print("-" * 30)
    try:
        attention_analyzer = MambaAttentionNeurons(model, enable_attention_computation=True)
        attention_data = attention_analyzer.extract_attention_vectors(inputs["input_ids"], [0, 2, 4])
        
        # Create neurons using different methods
        methods = ['attention_weighted', 'gradient_guided', 'rollout']
        for method in methods:
            neurons = attention_analyzer.create_mamba_neurons(attention_data, method)
            print(f"  ‚úÖ Created neurons using {method} method")
            
            # Analyze neuron behavior
            for layer_idx in [0, 2, 4]:
                if layer_idx in neurons:
                    analysis = attention_analyzer.analyze_neuron_behavior(neurons, layer_idx)
                    if analysis:
                        print(f"    Layer {layer_idx}: {analysis['num_neurons']} neurons, "
                              f"mean activation: {analysis['mean_activation']:.4f}")
        
    except Exception as e:
        print(f"  ‚ùå Error in attention neuron analysis: {e}")
    
    # 2. Layer Correlation Analysis
    print("\n2Ô∏è‚É£ Layer Correlation Analysis")
    print("-" * 30)
    try:
        layer_analyzer = LayerCorrelationAnalyzer(model, device)
        activations = layer_analyzer.extract_layer_activations([0, 2, 4], test_text)
        
        # Compute correlations
        correlations = layer_analyzer.compute_cross_layer_correlations()
        print(f"  ‚úÖ Computed correlations for {len(correlations)} layer pairs")
        
        # Analyze recursive patterns
        for layer_idx in [0, 2, 4]:
            if layer_idx in activations:
                patterns = layer_analyzer.analyze_recursive_patterns(layer_idx)
                print(f"    Layer {layer_idx}: analyzed recursive patterns")
        
    except Exception as e:
        print(f"  ‚ùå Error in layer correlation analysis: {e}")
    
    # 3. SSM Component Analysis
    print("\n3Ô∏è‚É£ SSM Component Analysis")
    print("-" * 30)
    try:
        ssm_extractor = SSMComponentExtractor(model, device)
        ssm_components = ssm_extractor.extract_ssm_components([0, 2, 4], test_text)
        
        # Analyze recursive dynamics
        for layer_idx in [0, 2, 4]:
            if layer_idx in ssm_components:
                dynamics = ssm_extractor.analyze_recursive_dynamics(layer_idx)
                print(f"    Layer {layer_idx}: analyzed recursive dynamics")
                
                if 'recursive_stability' in dynamics:
                    stability = dynamics['recursive_stability']
                    if stability['is_stable'] != 'unknown':
                        print(f"      Recursive stability: {'‚úÖ Stable' if stability['is_stable'] else '‚ùå Unstable'}")
        
    except Exception as e:
        print(f"  ‚ùå Error in SSM component analysis: {e}")
    
    print("\n‚úÖ Individual component analysis complete!")


def create_analysis_summary():
    """Create a summary of the analysis approach."""
    print("\nüìã ANALYSIS APPROACH SUMMARY")
    print("=" * 50)
    
    print("""
üß† Memory Recursion Neuron Analysis Approach:

1Ô∏è‚É£ ATTENTION NEURON EXTRACTION:
   - Extract attention vectors from Mamba layers
   - Create neurons using multiple methods (attention_weighted, gradient_guided, rollout)
   - Analyze neuron behavior and importance

2Ô∏è‚É£ RECURSIVE PATTERN ANALYSIS:
   - Extract layer activations across multiple layers
   - Compute temporal autocorrelation patterns
   - Analyze spatial correlation between dimensions
   - Study state evolution and memory effects

3Ô∏è‚É£ SSM COMPONENT CORRELATION:
   - Extract A, B, C matrices from SSM layers
   - Analyze recursive dynamics and stability
   - Correlate SSM components with neuron behavior
   - Study hidden state evolution

4Ô∏è‚É£ MEMORY EFFECTS ANALYSIS:
   - Extract delta parameters (memory modulation)
   - Analyze memory consistency and persistence
   - Study temporal variance patterns
   - Correlate memory effects with neuron activations

5Ô∏è‚É£ CROSS-LAYER CORRELATION:
   - Compute correlations between layers
   - Analyze information flow patterns
   - Study how memory recursion affects layer interactions

üéØ KEY INSIGHTS:
   - Memory recursion affects neuron behavior through delta parameters
   - Recursive patterns show temporal persistence in neuron activations
   - SSM stability correlates with neuron dynamics
   - Cross-layer correlations reveal information flow patterns
   - Attention weights interact with memory persistence
    """)


if __name__ == "__main__":
    print("üöÄ Memory Recursion Neuron Analysis")
    print("=" * 50)
    print("This script demonstrates how memory recursion affects layerwise neurons")
    print("calculated using attention_neurons.py")
    print("=" * 50)
    
    # Create analysis summary
    create_analysis_summary()
    
    # Run comprehensive analysis
    print("\n" + "="*70)
    print("üöÄ RUNNING COMPREHENSIVE ANALYSIS")
    print("="*70)
    
    analyzer, results = run_comprehensive_analysis()
    
    # Run individual component analysis
    print("\n" + "="*70)
    print("üîß RUNNING INDIVIDUAL COMPONENT ANALYSIS")
    print("="*70)
    
    run_individual_component_analysis()
    
    print("\n" + "="*70)
    print("‚úÖ ALL ANALYSES COMPLETE!")
    print("="*70)
    print("üìÅ Check the following files for results:")
    print("  - memory_recursion_analysis/ (visualizations)")
    print("  - memory_recursion_analysis_report.json (comprehensive report)")
    print("  - layer_correlation_analysis.json (if generated)")
    print("  - ssm_components_layer_*.png (if generated)")
    print("="*70)
